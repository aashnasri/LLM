{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX4ocn1wOd-S",
        "outputId": "3d32514a-0a7b-4e0c-ab8f-26846635e397"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai (from -r requirements.txt (line 1))\n",
            "  Downloading openai-1.25.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain (from -r requirements.txt (line 2))\n",
            "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit (from -r requirements.txt (line 3))\n",
            "  Downloading streamlit-1.34.0-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from -r requirements.txt (line 4))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting PyPDF2 (from -r requirements.txt (line 5))\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r requirements.txt (line 1)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 1))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.36 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.48 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading langsmith-0.1.53-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (8.2.3)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->-r requirements.txt (line 3)) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (8.1.7)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (24.0)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (14.0.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (13.7.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (0.10.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit->-r requirements.txt (line 3))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit->-r requirements.txt (line 3))\n",
            "  Downloading pydeck-0.9.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 3)) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit->-r requirements.txt (line 3))\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 3)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 3)) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 3)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (1.2.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 2))\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 2))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 3))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain->-r requirements.txt (line 2))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<25,>=16.8 (from streamlit->-r requirements.txt (line 3))\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2))\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit->-r requirements.txt (line 3)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit->-r requirements.txt (line 3)) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 3)) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 2)) (3.0.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 3))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit->-r requirements.txt (line 3)) (2.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 3)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 3)) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 3)) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r requirements.txt (line 3)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit->-r requirements.txt (line 3)) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 2))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: watchdog, smmap, python-dotenv, PyPDF2, packaging, orjson, mypy-extensions, jsonpointer, h11, typing-inspect, pydeck, marshmallow, jsonpatch, httpcore, gitdb, langsmith, httpx, gitpython, dataclasses-json, openai, langchain-core, streamlit, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed PyPDF2-3.0.1 dataclasses-json-0.6.5 gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.17 langchain-community-0.0.36 langchain-core-0.1.50 langchain-text-splitters-0.0.1 langsmith-0.1.53 marshmallow-3.21.2 mypy-extensions-1.0.0 openai-1.25.1 orjson-3.10.3 packaging-23.2 pydeck-0.9.0 python-dotenv-1.0.1 smmap-5.0.1 streamlit-1.34.0 typing-inspect-0.9.0 watchdog-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import traceback\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SequentialChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import PyPDF2"
      ],
      "metadata": {
        "id": "XtnBhHCRO0R9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mykey = \"******\""
      ],
      "metadata": {
        "id": "jqI5E-zfO5vw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatOpenAI(openai_api_key=mykey,model_name=\"gpt-3.5-turbo\", temperature=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OJRSgFQO5k6",
        "outputId": "ef01fffd-8aa9-462a-f2a1-151deadb9ed0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader = PyPDF2.PdfReader('Aashna_cv.pdf')"
      ],
      "metadata": {
        "id": "GEpFoJj1jmSr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(reader.pages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okZngJFOj8OL",
        "outputId": "a7690f12-4998-4c89-d1ec-191ff17892d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reader.pages[0].extract_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyK-jFo4jyUc",
        "outputId": "64fb9af1-ea85-4bd0-81d3-6b26cc529124"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "   meaashna.24@gmail.com  \n",
            " \n",
            "8882483854, 8447427388  \n",
            " \n",
            "Noida,  India  \n",
            " \n",
            "linkedin.com/in/aashna -\n",
            "srivastava -774438267  \n",
            " \n",
            "SKILLS  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "INTERESTS  \n",
            " \n",
            " \n",
            " WORK  EXPERIENCE  \n",
            " \n",
            "Senior Consultant - Data Science\n",
            "Ernst & Young LLP\n",
            "02/2022 - Present       Noida\n",
            "Tasks\n",
            " Implemented machine learning predictive model in Azure Databricks to automate\n",
            "demand planning process and supply chain analytics for various European markets for a leading \n",
            "global FMCG.\n",
            " Built forecasting models using an ensemble of algorithms like XGboost, Random Forest,\n",
            "ARIMA to have a better view on the long term forecast which can optimize the supply chain \n",
            "solutions.\n",
            " Improved demand forecasting accuracy by 8-10% compared to traditional methods,\n",
            "optimizing resource allocation.\n",
            " Streamlined data ingestion for a leading Pharma client using a multi-step pipeline and parquet\n",
            "database, then leveraged Spark and Python for analysis and automation. This included pre-processing, \n",
            "SQL queries, ETL pipeline creation, and automated data quality checks, culminating in a cloud-based \n",
            "reporting app accessible to external users.\n",
            " Marketing Mix Modelling - Worked on optimizing the channel spends using LPP to identify the key sales\n",
            "driver impacting the ROI, measure promotional impact and the contribution of each channel towards \n",
            "overall revenue.\n",
            " Responsible for the Pricing Analytics Architecture from generating and \n",
            "manipulating synthetic data to Customer Segmentation using K- Means\n",
            "clustering. Dynamic Pricing along with Demand Forecasting using Light GBM. \n",
            " Applied machine learning algorithms to deliver pricing solutions like Customer\n",
            "segmentation, customer lifetime value, customer churn prediction, demand \n",
            "forecasting, price elasticity and price optimization.\n",
            " Analyzed procurement purchase orders with stats (Hypothesis testing, z-score, \n",
            "skewness, multi-variate analysis) and classification models to identify predictors\n",
            "of order cancellation and compare their performance. \n",
            "Senior Developer\n",
            "HCL Technologies\n",
            "03/2019 - 11/2021        Noida\n",
            "Achievements/Tasks\n",
            " Performed exploratory data analysis using python libraries like pandas, Sklearn, SciPy for data\n",
            "science on bank stock prices and their predictive modeling.\n",
            " Predictive Analysis of the insurance industry data using the Random Forest model to derive conclusive\n",
            "results if a loan could be sanctioned to a customer based on his credit behavior.\n",
            " Analyzed user behavior for advertisement data to determine whether a particular visited an ad website\n",
            "based on features using Logistic Regression Models.\n",
            " Customer segmentation to understand the target potential customers and identify their common characteristics using Machine Learning Models as a step to expand business.\n",
            "System Analyst\n",
            "DXC Technology\n",
            "10/2015 - 03/2019 Noida\n",
            "Tasks\n",
            " Compilation and manual testing of Server Management tools such as openssh, Perl, python, sudo, \n",
            "rsync, syslog-ng, lsof on various Linux and Unix servers. Created scripts in shell for automating the \n",
            "testing of SM tools.\n",
            " Development of the PowerShell Script using WMI (Windows Measurement Instrumentation) classes\n",
            "and their attributes to design WinConnect tool to gather system information namely CPU data, Disk \n",
            "Drive Data, Networking Data, Page File Data.\n",
            " \n",
            "EDUCATION  \n",
            "B TECH  in Elect ronics & Communication E ngineering  \n",
            "Inderprastha Engineering College, Uttar Pradesh Technical University  \n",
            "08/201 1 - 05/201 5 75% \n",
            " \n",
            "Higher  Secondary  \n",
            "St. Basil’s School  \n",
            "06/20 09 - 06/2011 90% \n",
            "Machine  Learning  \n",
            " NLP \n",
            " \n",
            "Power  BI \n",
            "Aashna Srivastava \n",
            "Data Science & Analytics\n",
            "A data science/analytics consultant with total 8 years of experience in the IT industry and a relevant experience of 5 years in Data Science and Analytics \n",
            "solution designing and development.\n",
            " \n",
            "SQL \n",
            " \n",
            " \n",
            "Data  Analytics  \n",
            " \n",
            " Excel  \n",
            "  \n",
            "Pyspark  \n",
            " Azure  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume = reader.pages[0].extract_text()"
      ],
      "metadata": {
        "id": "TmhibUmqRFLn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPLATE=\"\"\"\n",
        "Resume:{resume}\n",
        "You are an expert resume reviewer, Given the above resume, it is your job to \\\n",
        "decide whether a resume is suitable for {job}. Give review of the resume as well\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CT_KF16Lj4Mv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_prompt = PromptTemplate(\n",
        "    input_variables=[\"resume\",\"job\"],\n",
        "    template=TEMPLATE\n",
        "    )"
      ],
      "metadata": {
        "id": "nGvER6XWRu75"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_chain=LLMChain(llm=llm, prompt=resume_prompt, output_key=\"review\", verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x06TYAKORuwh",
        "outputId": "e88abeb2-a76f-4e97-a83a-56448a74eba4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_chain = SequentialChain(chains=[review_chain], input_variables=['resume','job'],\n",
        "                                        output_variables=[\"review\"], verbose=True,)"
      ],
      "metadata": {
        "id": "k6CxXbWDTZqI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = seq_chain(\n",
        "        {\n",
        "            \"resume\": resume,\n",
        "            \"job\": 'Data Scientist'\n",
        "\n",
        "        }\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqNG21mQU_S1",
        "outputId": "081420fd-b4bb-4bee-d864-a7c1318f620c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Resume: \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "   meaashna.24@gmail.com  \n",
            " \n",
            "8882483854, 8447427388  \n",
            " \n",
            "Noida,  India  \n",
            " \n",
            "linkedin.com/in/aashna -\n",
            "srivastava -774438267  \n",
            " \n",
            "SKILLS  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "INTERESTS  \n",
            " \n",
            " \n",
            " WORK  EXPERIENCE  \n",
            " \n",
            "Senior Consultant - Data Science\n",
            "Ernst & Young LLP\n",
            "02/2022 - Present       Noida\n",
            "Tasks\n",
            " Implemented machine learning predictive model in Azure Databricks to automate\n",
            "demand planning process and supply chain analytics for various European markets for a leading \n",
            "global FMCG.\n",
            " Built forecasting models using an ensemble of algorithms like XGboost, Random Forest,\n",
            "ARIMA to have a better view on the long term forecast which can optimize the supply chain \n",
            "solutions.\n",
            " Improved demand forecasting accuracy by 8-10% compared to traditional methods,\n",
            "optimizing resource allocation.\n",
            " Streamlined data ingestion for a leading Pharma client using a multi-step pipeline and parquet\n",
            "database, then leveraged Spark and Python for analysis and automation. This included pre-processing, \n",
            "SQL queries, ETL pipeline creation, and automated data quality checks, culminating in a cloud-based \n",
            "reporting app accessible to external users.\n",
            " Marketing Mix Modelling - Worked on optimizing the channel spends using LPP to identify the key sales\n",
            "driver impacting the ROI, measure promotional impact and the contribution of each channel towards \n",
            "overall revenue.\n",
            " Responsible for the Pricing Analytics Architecture from generating and \n",
            "manipulating synthetic data to Customer Segmentation using K- Means\n",
            "clustering. Dynamic Pricing along with Demand Forecasting using Light GBM. \n",
            " Applied machine learning algorithms to deliver pricing solutions like Customer\n",
            "segmentation, customer lifetime value, customer churn prediction, demand \n",
            "forecasting, price elasticity and price optimization.\n",
            " Analyzed procurement purchase orders with stats (Hypothesis testing, z-score, \n",
            "skewness, multi-variate analysis) and classification models to identify predictors\n",
            "of order cancellation and compare their performance. \n",
            "Senior Developer\n",
            "HCL Technologies\n",
            "03/2019 - 11/2021        Noida\n",
            "Achievements/Tasks\n",
            " Performed exploratory data analysis using python libraries like pandas, Sklearn, SciPy for data\n",
            "science on bank stock prices and their predictive modeling.\n",
            " Predictive Analysis of the insurance industry data using the Random Forest model to derive conclusive\n",
            "results if a loan could be sanctioned to a customer based on his credit behavior.\n",
            " Analyzed user behavior for advertisement data to determine whether a particular visited an ad website\n",
            "based on features using Logistic Regression Models.\n",
            " Customer segmentation to understand the target potential customers and identify their common characteristics using Machine Learning Models as a step to expand business.\n",
            "System Analyst\n",
            "DXC Technology\n",
            "10/2015 - 03/2019 Noida\n",
            "Tasks\n",
            " Compilation and manual testing of Server Management tools such as openssh, Perl, python, sudo, \n",
            "rsync, syslog-ng, lsof on various Linux and Unix servers. Created scripts in shell for automating the \n",
            "testing of SM tools.\n",
            " Development of the PowerShell Script using WMI (Windows Measurement Instrumentation) classes\n",
            "and their attributes to design WinConnect tool to gather system information namely CPU data, Disk \n",
            "Drive Data, Networking Data, Page File Data.\n",
            " \n",
            "EDUCATION  \n",
            "B TECH  in Elect ronics & Communication E ngineering  \n",
            "Inderprastha Engineering College, Uttar Pradesh Technical University  \n",
            "08/201 1 - 05/201 5 75% \n",
            " \n",
            "Higher  Secondary  \n",
            "St. Basil’s School  \n",
            "06/20 09 - 06/2011 90% \n",
            "Machine  Learning  \n",
            " NLP \n",
            " \n",
            "Power  BI \n",
            "Aashna Srivastava \n",
            "Data Science & Analytics\n",
            "A data science/analytics consultant with total 8 years of experience in the IT industry and a relevant experience of 5 years in Data Science and Analytics \n",
            "solution designing and development.\n",
            " \n",
            "SQL \n",
            " \n",
            " \n",
            "Data  Analytics  \n",
            " \n",
            " Excel  \n",
            "  \n",
            "Pyspark  \n",
            " Azure  \n",
            "You are an expert resume reviewer, Given the above resume, it is your job to decide whether a resume is suitable for Data Scientist. Give review of the resume as well\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.get(\"review\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "ol4BrIcMWq8-",
        "outputId": "37fd655a-e7b6-4b5e-ac07-f970f3b3d017"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Review: \\n\\nOverall, this resume is well-suited for a Data Scientist position. The candidate has relevant experience in Data Science and Analytics, with a strong background in machine learning, predictive modeling, and data analysis. They have worked on a variety of projects involving demand forecasting, pricing analytics, customer segmentation, and procurement analysis. \\n\\nThe candidate also has experience with a variety of tools and technologies commonly used in data science, such as Python, SQL, Power BI, Pyspark, and Azure. They have a solid educational background with a B.Tech in Electronics & Communication Engineering and have also completed courses in Machine Learning and NLP.\\n\\nOne suggestion for improvement would be to provide more specific details about the impact of their work, such as quantifying the results achieved in terms of improved accuracy, cost savings, or other key metrics. Additionally, including any certifications or additional training related to data science could further strengthen the resume. \\n\\nOverall, this candidate appears to have the necessary skills and experience to excel in a Data Scientist role.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}